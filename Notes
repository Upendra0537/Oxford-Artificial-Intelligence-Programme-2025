Notes:

Module 1: Artificial Intelligence EcoSystem

Turing Test
https://youtu.be/3wLqsRLvV-c?si=VNenakrwMT_t18dq

AI Def:
The study of the design of intelligent agents	- Acting rationally 
The process of developing machines that perform functions that would normally require human intelligence to be performed - Acting humanly 
The study of the ability of a computer to eventually perceive, reason, and act - Thinking rationally
The exciting new effort to make computers think, as if they had minds	- Thinking humanly
 
GPT - General Purpose Technology
Characteristics
Pervasiveness: The technology spreads across most sectors.
Improvement: It will improve in quality over time and will continue to lower the costs of use.
Innovation: It enables other inventions.

Hype Cycle
Innovation trigger: The first stage in the cycle represents the period when the technology is first invented, which sparks excitement. This is usually triggered by media coverage resulting from a public announcement or a physical demonstration of the technology.
Peak of inflated expectation: During this period, hype and excitement builds, and optimism and imagination far exceeds the reality of the potential the technology holds. This stage is also known for investment into the technology without an understanding of it, and without a sound business case for the investment decision.
Trough of disillusionment: This period is when hope and faith in the technology is lost, because it is not performing as well as expected. Negative media coverage is widespread.
Slope of enlightenment: Confidence in the technology is regained, and the potential for further application of the technology becomes clear.
Plateau of productivity: During the maturity stage, growth slows down, the technology is realistically valued, and adoption accelerates.


Module 2: AI and machine learning: Understanding the black box

Types of Data
- Structured
- Unstructured

Computational Limits -  P & NP Problem, Traveling Salesman Problem

Three learning approaches 
– supervised 
 - Regression - Linear (Continous Output)
 - Classification - Logistic (Binary Output)
- unsupervised
 - Clustering
- reinforcement learning 

Four main stages of ML:
Manage data: data is collected, prepared, and split for training and testing
Train model: the task, features, and algorithms are chosen, and the model is trained
Evaluate model: the trained model is assessed and improved
Deploy model: the trained model is deployed for prediction on new data, and the model’s performance is monitored and eventually re-retrained

Tensor Flow - Movie Review - Google Colab - https://www.tensorflow.org/tutorials/keras/text_classification_with_hub

Module 3: Understanding deep learning and neural networks

Neural Network
- Input
- Hidden layer - activation Function
- Output

- Weights - 
- Error Rate - 

Training data set: This set enables ANNs to establish the appropriate weights between the neurons in the different layers.
Validation data set: This is used to refine the ANN’s processing ability.
Test data set: This is used to test whether the ANN has the ability to process the input into an accurate output.

Forward propagation - Forward propagation entails signals moving from the input on the left to the output on the right
Backpropagation - backpropagation in ANNs functions by adjusting or correcting the weights between neurons in order to minimise the error in the output of the ANN, and is the process by which an ANN learns

Module 4: Beyond prediction: Making the most of generative AI
Predictive AI
Generative AI
 - Tokenisation
 - Vector embeddings
 - Attention mechanism
 - Multimodality

- Large language models (LLMs): LLMs are very large and powerful text-to-text systems usually using transformer networks. These can vary in size but typically contain hundreds of billions to trillions of parameters. It’s worth noting that the human brain has around 200 trillion synapses.
- Small language models (SLMs): SLMs are smaller text-to-text systems that still tend to be very performant compared to the larger LLMs. Typically, SLMs will contain a few billion to tens of billions of parameters, so they are approximately 100 times smaller than LLMs.
- Large multimodal models (LMMs): LMMs are models that accept various input formats and deliver a similar variety of output types. They are more versatile than LLMs as they can process different types of information including text, audio, images, and videos.
- Small multimodal models (SMMs): SMMs are similar to LMMs but with less parameters. Similarly, they are more versatile than SLMs.
- Foundation model: These are deep learning models that are pre-trained on vast amounts of publicly available data, usually from the internet. They can include all four of the aforementioned categories of models.
- Frontier model: Frontier models refer to the latest types of AI models. They are foundation models but can also include new types of architectures and approaches. It is a term often used by policy makers and regulators to describe the most powerful models available.

Agentic AI - Agentic AI refers to an autonomous, goal-directed AI system (consisting of AI agents) that incorporates the use of advanced reasoning and iterative planning to solve elaborate problems within dynamic environments

Transformers
- Input processing
- Embedding
- Positional encoding
- Self-attention
- Multi-head attention
- Feed-forward networks
- Layer normalisation and residual connections
- Decoder stack
- Output layer
- Softmax
- Selection

Limitations of Gen AI
- Hallucination - Misinformation /Disinformation
- Copyright Violation
- Security Issues

Prompt Engineering
Retrieval augmented generation
Fine-tuning an open-source model
Pretrain your own genAI Model

Module 5: AI and society

- Ethical AI: AI systems that adhere to ethical principles and thus ensure that these systems are aligned with social norms and values.
- Safe AI: A key concern is that AI systems may cause physical and psychological harm to those affected by its output, either directly or indirectly.
- Responsible AI: While similar to ethical AI, the idea here is that AI should be used in a responsible way that does not cause any harm. While ethical AI focuses on the underlying principles, responsible AI focuses on the practices, policies, and governance of AI.
- Explainable AI: It is difficult, if not impossible, to understand how a certain prediction has been reached in complex AI systems. Explainable AI promotes practices that alleviate this problem. For example, using simulations, What-if scenarios, and counterfactuals.
- Robust AI: Robustness is a technical requirement, arguing that AI models should produce consistent output even in extreme or edge cases, reducing the risk of unintended consequences.
- Trustworthy AI: The concept of trustworthiness was introduced by the EU to bring together three key requirements that AI systems should fulfil:Legal compliance, for example, with privacy laws such as the GDPR Technical robustness, to ensure consistent performance of the model Ethical soundness, to ensure adherence to social norms and values

AI Failures
- Privacy
- Intellectual property rights
- Explainability - Fairlearn Tool and What-if Tool
- Misinformation - AI language model errors, Outdated AI information, Misinterpreted AI research
- disinformation - Deepfakes, AI-generated fake news, Malicious chatbots, AI-enhanced phishing
- Bias
  - Algorithmic Bias

AI ethics
- Unacceptable risk (banned outright)
- High risk (subject to strict obligations)
- Limited risk (transparency requirements only)
- Minimal risk (no compliance mandate)

- AI Act - EU
- National AI Initiative Act of 2020 - USA
- International Organization for Standardization (ISO)
 - ISO 42001 on AI management systems, and the technical report 
 - ISO 24027 on bias in AI and AI-aided decision-making
- United Nations Educational, Scientific and Cultural Organization (UNESCO)
- Organisation for Economic Co-operation and Development (OECD)

Tool - capAI – A conformity assessment procedure for AI system -  to trak all the AI models, their risks.
